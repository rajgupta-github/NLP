{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Problem in HMM\n",
    "\n",
    "Given a sequence of visible symbol 𝑉𝑇 and the model ( 𝜃→{𝐴,𝐵} ) find the most probable sequence of hidden states 𝑆𝑇.\n",
    "\n",
    "In general we could try to find all the different scenarios of hidden states for the given sequence of visible symbols and then identify the most probable one. However, just like we have seen earlier, it will be an exponentially complex problem 𝑂(𝑁𝑇.𝑇) to solve.\n",
    "\n",
    "# Viterbi Algorithm\n",
    "\n",
    "We will be using a much more efficient algorithm named Viterbi Algorithm to solve the decoding problem. So far in HMM we went deep into deriving equations for all the algorithms in order to understand them clearly. However Viterbi Algorithm is best understood using an analytical example rather than equations. I will provide the mathematical definition of the algorithm first, then will work on a specific example.\n",
    "\n",
    "## Probabilistic View\n",
    "\n",
    "The decoding problem is similar to the Forward Algorithm. In Forward Algorithm we compute the likelihood of the observation sequence, given the hidden sequences by summing over all the probabilities, however in decoding problem we need to find the most probable hidden state in every iteration of t.\n",
    "\n",
    "The following equation represents the highest probability along a single path for first t observations which ends at state i. \n",
    "\n",
    "![alt](hmmimg/v1.png)\n",
    "\n",
    "We can use the same approach as the Forward Algorithm to calculate 𝜔𝑖(+1)\n",
    "\n",
    "![alt](hmmimg/v2.png)\n",
    "\n",
    "Now to find the sequence of hidden states we need to identify the state that maximizes 𝜔𝑖(𝑡) at each time step t.\n",
    "\n",
    "![alt](hmmimg/v3.png)\n",
    "\n",
    "Once we complete the above steps for all the observations, we will first find the last hidden state by maximum likelihood, then using backpointer to backtrack the most likely hidden path.\n",
    "\n",
    "Everything what I said above may not make a lot of sense now. Go through the example below and then come back to read this part. I hope it will definitely be more easy to understand once you have the intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Our example will be same one used in during programming, where we have two hidden states A,B and three visible symbols 1,2,3. Assume we have a sequence of 6 visible symbols and the model 𝜃. We need to predict the sequence of the hidden states for the visible symbols.\n",
    "\n",
    "If we draw the trellis diagram, it will look like the fig 1. Note, here 𝑆1=𝐴 and 𝑆2=𝐵.\n",
    "\n",
    "As stated earlier, we need to find out for every time step t and each hidden state what will be the most probable next hidden state.\n",
    "\n",
    "Assume when t = 2, the probability of transitioning to 𝑆2(2) from 𝑆1(1) is higher than transitioning to 𝑆1(2), so we keep track of this. This is highlighted by the red arrow from 𝑆1(1) to 𝑆2(2) in the below diagram. The other path is in gray dashed line, which is not required now.\n",
    "\n",
    "Like wise, we repeat the same for each hidden state. In other words, assuming that at t=1 if 𝑆2(1) was the hidden state and at t=2 the probability of transitioning to 𝑆1(2) from 𝑆2(1) is higher, hence its highlighted in red.\n",
    "\n",
    "![alt](hmmimg/v4.webp)\n",
    "\n",
    "We can repeat the same process for all the remaining observations. The trellis diagram will look like following. \n",
    "\n",
    "\n",
    "![alt](hmmimg/v5.webp)\n",
    "\n",
    "The output of the above process is to have the sequences of the most probable states (1) [below diagram] and the corresponding probabilities (2). So as we go through finding most probable state (1) for each time step, we will have an 2x5 matrix ( in general M x (T-1) ) as below:\n",
    "\n",
    "![alt](hmmimg/v6.webp)\n",
    "\n",
    "The first number 2 in above diagram indicates that current hidden step 1 (since it’s in 1st row) transitioned from previous hidden step 2.\n",
    "\n",
    "Let’s take one more example, the 2 in the 2nd row 2nd col indicates that the current step 2 ( since it’s in 2nd row) transitioned from previous hidden step 2. If you refer fig 1, you can see its true since at time 3, the hidden state 𝑆2 transisitoned from 𝑆2 [ as per the red arrow line]\n",
    "\n",
    "Similar to the most probable state ( at each time step ), we will have another matrix of size 2 x 6 ( in general M x T ) for the corresponding probabilities (2). Next we find the last step by comparing the probabilities(2) of the T’th step in this matrix.\n",
    "\n",
    "Assume, in this example, the last step is 1 ( A ), we add that to our empty path array. then we find the previous most probable hidden state by backtracking in the most probable states (1) matrix. Refer the below fig 3 for the derived most probable path.The path could have been different if the last hidden step was 2 ( B ).\n",
    "\n",
    "![alt](hmmimg/v7.webp)\n",
    "\n",
    "The final most probable path in this case is given in the below diagram, which is similar as defined in fig 1.\n",
    "\n",
    "![alt](hmmimg/v8.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
