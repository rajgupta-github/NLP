{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Markov Model (hereinafter referred to as HMM) is a relatively classic machine learning model. It has been widely used in the fields of language recognition, natural language processing, and pattern recognition.\n",
    "\n",
    "Of course, with the current rise of deep learning, especially the popularity of neural network sequence models such as RNN , LSTM , HMM's status has declined.\n",
    "\n",
    "But as a classic model, learning HMM models and corresponding algorithms improves our ability to solve problem modeling and expand our algorithmic ideas.\n",
    "\n",
    "\n",
    "#### What kind of problems need the HMM model\n",
    "\n",
    "First let's see what kind of problem solving can use HMM model.\n",
    "\n",
    "Our problems when using the HMM model generally have these two characteristics:\n",
    "\n",
    "1) Our problem is sequence-based, such as time series or state series.\n",
    "\n",
    "2) There are two types of data in our problem. One type of sequence data is observable, that is, the observation sequence; the other type of data is not observable, that is, the hidden state sequence, referred to as the state sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The HMM is a sequence model.  A sequence model or sequence classifier is a model whose job is to assign a label or class to each unit in a sequence,thus mapping a sequence of observations to a sequence of labels.  An HMM is aprobabilistic sequence model: given a sequence of units (words, letters, morphemes,sentences, whatever), it computes a probability distribution over possible sequencesof labels and chooses the best label sequence.\n",
    "\n",
    "\n",
    "### Markov Chains\n",
    "\n",
    "\n",
    "A Markov chain is a model that  tells  us  something  about  the  probabilities  of  sequences  of  random variables, states, each of which can take on values from some set. These sets can be words, ortags, or symbols representing anything, for example the weather.  A Markov chainmakes a very strong assumption that if we want to predict the future in the sequence, all that matters is the current state. All the states before the current state have no impact on the future except via the current state. \n",
    "\n",
    "It’s as if to predict tomorrow’s weather you could examine today’s weather but you weren’t allowed to look at yesterday’s weather.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a sequence of state variables q1,q2,...,qi.  A Markov model embodies the Markov assumption on the probabilities of this sequence: that when predicting the future, the past doesn’t matter, only the present.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically we can say, the probability of the state at time t will only depend on time step t-1. In other words, probability of s(t) given s(t-1), that is 𝑝(𝑠(𝑡)|𝑠(𝑡−1)). This is known as First Order Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case, the probability of the state s at time t depends on time step t-1 and t-2, it’s known as 2nd Order Markov Model. As you increase the dependency of past time events the order increases. The 2nd Order Markov Model can be written as 𝑝(𝑠(𝑡)|𝑠(𝑡−1),𝑠(𝑡−2))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, the idea is to model the joint probability, such as the probability of 𝑠𝑇={𝑠1,𝑠2,𝑠3} where s1, s2 and s3 happens sequentially. We can use the joint & conditional probability rule and write it as:\n",
    "\n",
    "<center>\n",
    "𝑝(𝑠3,𝑠2,𝑠1)=𝑝(𝑠3|𝑠2,𝑠1)𝑝(𝑠2,𝑠1)\n",
    "           =𝑝(𝑠3|𝑠2,𝑠1)𝑝(𝑠2|𝑠1)𝑝(𝑠1)\n",
    "           =𝑝(𝑠3|𝑠2)𝑝(𝑠2|𝑠1)𝑝(𝑠1)\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the diagram of a simple Markov Model as we have defined in above equation.\n",
    "\n",
    "![alt](hmmimg/hmm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition Probabilities:\n",
    "\n",
    "The probability of one state changing to another state is defined as Transition Probability. So in case there are 3 states (Sun, Cloud, Rain) there will be total 9 Transition Probabilities.As you see in the diagram, we have defined all the Transition Probabilities. Transition Probability generally are denoted by 𝑎𝑖𝑗 which can be interpreted as the Probability of the system to transition from state i to state j at time step t+1.\n",
    "\n",
    "Mathematically,\n",
    "\n",
    "<center><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <msub>\n",
    "    <mi>a</mi>\n",
    "    <mrow class=\"MJX-TeXAtom-ORD\">\n",
    "      <mi>i</mi>\n",
    "      <mi>j</mi>\n",
    "    </mrow>\n",
    "  </msub>\n",
    "  <mo>=</mo>\n",
    "  <mi>p</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mtext>&#xA0;</mtext>\n",
    "  <mi>s</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mi>t</mi>\n",
    "  <mo>+</mo>\n",
    "  <mn>1</mn>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>=</mo>\n",
    "  <mi>j</mi>\n",
    "  <mtext>&#xA0;</mtext>\n",
    "  <mrow class=\"MJX-TeXAtom-ORD\">\n",
    "    <mo stretchy=\"false\">|</mo>\n",
    "  </mrow>\n",
    "  <mtext>&#xA0;</mtext>\n",
    "  <mi>s</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mi>t</mi>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>=</mo>\n",
    "  <mi>i</mi>\n",
    "  <mtext>&#xA0;</mtext>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "</math></center>\n",
    "\n",
    "\n",
    "![alt](hmmimg/hmm2.png)\n",
    "\n",
    "For an example, in the above state diagram, the Transition Probability from Sun to Cloud is defined as 𝑎12. Note that, the transition might happen to the same state also. This is also valid scenario. If we have sun in two consecutive days then the Transition Probability from sun to sun at time step t+1 will be 𝑎11.\n",
    "\n",
    "Generally, the Transition Probabilities are define using a (M x M) matrix, known as Transition Probability Matrix. We can define the Transition Probability Matrix for our above example model as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "        |a11 a12 a13|\n",
    "     A =|a21 a22 a23|\n",
    "        |a31 a32 a33|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once important property to notice, when the machine transitions to another state, the sum of all transition probabilities given the current state should be 1. In our example 𝑎11+𝑎12+𝑎13 should be equal to 1.\n",
    "\n",
    "Mathematically,\n",
    "\n",
    "![alt](hmmimg/hmm3.png)\n",
    "\n",
    "Initial Probability Distribution:\n",
    "\n",
    "The machine/system has to start from one state. The initial state of Markov Model ( when time step t = 0) is denoted as 𝜋, it’s a M dimensional row vector. All the probabilities must sum to 1.During implementation, we can just assign the same probability to all the states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Chain\n",
    "\n",
    "There are basic 4 types of Markov Models. When the system is fully observable and autonomous it’s called as Markov Chain. What we have learned so far is an example of Markov Chain. Hence we can conclude that Markov Chain consists of following parameters:\n",
    "\n",
    "-  A set of M states\n",
    "-  A transition probability matrix A\n",
    "-  An initial probability distribution 𝜋\n",
    "\n",
    "#### Final/Absorbing State\n",
    "\n",
    "When the transition probabilities of any step to other steps are zero except for itself then its knows an Final/Absorbing State.So when the system enters into the Final/Absorbing State, it never leaves. \n",
    "\n",
    "### Hidden Markov Model\n",
    "\n",
    "In Hidden Markov Model the state of the system will be hidden (unknown), however at every time step t the system in state s(t) will emit an observable/visible symbol v(t).You can see an example of Hidden Markov Model in the below diagram.\n",
    "\n",
    "![alt](hmmimg/hmm4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emission Probability\n",
    "\n",
    "Now, let’s redefine our previous example. Assume based on the weather of any day the mood of a person changes from happy to sad. Also assume the person is at a remote place and we do not know how is the weather there. We can only know the mood of the person. So in this case, weather is the hidden state in the model and mood (happy or sad) is the visible/observable symbol. So we should be able to predict the weather by just knowing the mood of the person using HMM. If we redraw the states it would look like this:\n",
    "\n",
    "![alt](hmmimg/hmm5.png)\n",
    "\n",
    "The observable symbols are {𝑣1,𝑣2}, one of which must be emitted from each state. The probability of emitting any symbol is known as Emission Probability, which are generally defined as 𝑏𝑗𝑘. Mathematically, the probability of emitting symbol k given state j.\n",
    "\n",
    "![alt](hmmimg/hmm6.png)\n",
    "\n",
    "Emission probabilities are also defined using MxC matrix, named as Emission Probability Matrix.\n",
    "\n",
    "\n",
    "        |b11 a12|\n",
    "     B =|b21 a22|\n",
    "        |b31 a32|\n",
    "        \n",
    "        \n",
    "Again, just like the Transition Probabilities, the Emission Probabilities also sum to 1. \n",
    "\n",
    "\n",
    "So far we have defined different attributes/properties of Hidden Markov Model. Prediction is the ultimate goal for any model/algorithm. However before jumping into prediction we need to solve two main problem in HMM.\n",
    "\n",
    "### Issues with Hidden Markov Model\n",
    "\n",
    "1. **Evaluation Problem**\n",
    "\n",
    "Let’s first define the model ( 𝜃 ) as following:\n",
    "𝜃→𝑠,𝑣,𝑎𝑖𝑗,𝑏𝑗𝑘\n",
    "\n",
    "Given the model ( 𝜃 ) and Sequence of visible/observable symbol ( 𝑉𝑇 ), we need to determine the probability that a particular sequence of visible states/symbol ( 𝑉𝑇 ) that was generated from the model ( 𝜃 ).\n",
    "\n",
    "There could be many models {𝜃1,𝜃2…𝜃𝑛}. We need to find 𝑝(𝑉𝑇|𝜃𝑖), then use Bayes Rule to correctly classify the sequence 𝑉𝑇.\n",
    "\n",
    "𝑝(𝜃|𝑉𝑇)=𝑝(𝑉𝑇|𝜃)𝑝(𝜃)𝑝(𝑉𝑇)\n",
    "\n",
    "\n",
    "2. **Learning Problem**\n",
    "\n",
    "HMM is unsupervised learning process, where number of different visible symbol types are known (happy, sad etc), however the number of hidden states are not known. The idea is to try out different options, however this may lead to more computation and processing time.\n",
    "\n",
    "Hence we often use training data and specific number of hidden states (sun, rain, cloud etc) to train the model for faster and better prediction.\n",
    "\n",
    "Once the high-level structure (Number of Hidden & Visible States) of the model is defined, we need to estimate the Transition (𝑎𝑖𝑗) & Emission (𝑏𝑗𝑘) Probabilities using the training sequences. This is known as the Learning Problem.\n",
    "\n",
    "3. **Decoding Problem**\n",
    "\n",
    "Finally, once we have the estimates for Transition (𝑎𝑖𝑗) & Emission (𝑏𝑗𝑘) Probabilities, we can then use the model ( 𝜃 ) to predict the Hidden States 𝑊𝑇 which generated the Visible Sequence 𝑉𝑇. The Decoding Problem is also known as Viterbi Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
