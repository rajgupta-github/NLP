{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important and complex part of Hidden Markov Model is the Learning Problem. Even though it can be used as Unsupervised way, the more common approach is to use Supervised learning just for defining number of hidden states.\n",
    "\n",
    "Learning Problem : HMM Training\n",
    "\n",
    "-    The objective of the Learning Problem is to estimate for ùëéùëñùëó and ùëèùëóùëò using the training data.\n",
    "-    The standard algorithm for Hidden Markov Model training is the Forward-Backward or Baum-Welch Algorithm.\n",
    "-    This algorithm uses a special case of the Expectation Maximization (EM) Algorithm.\n",
    "\n",
    "\n",
    "Example using Maximum Likelihood Estimate:\n",
    "\n",
    "Now let‚Äôs try to get an intuition using an example of Maximum Likelihood Estimate.Consider training a Simple Markov Model where the hidden state is visible.\n",
    "\n",
    "We we use our example used in the programming section (You should already have it if you have followed part 2) where we had 2 hidden states [A,B] and 3 visible states [1,2,3]. (Assume in this example the hidden states are also known)\n",
    "\n",
    "As you see here we have 4 different sets of sequences (each in alternative colors).\n",
    "\n",
    "![alt](hmmimg/bw1.png)\n",
    "\n",
    "Now we will compute the HMM parameters by Maximum Likelihood Estimation using the sample data above.\n",
    "\n",
    "### Estimate Initial Probability Distribution\n",
    "\n",
    "We will initialize ùúã using the probability derived from the above sequences. In the example above, one of the sequence started with A and rest all 3 with B. We can define,\n",
    "\n",
    "![alt](hmmimg/bw2.png)\n",
    "\n",
    "### Estimate Transition Probabilities:\n",
    "\n",
    "Lets define our Transition Probability Matrix first as:\n",
    "\n",
    "![alt](hmmimg/bw3.png)\n",
    "\n",
    "We can calculate the probabilities from the example as (Ignore the final hidden state since there is to state to transition to):\n",
    "\n",
    "![alt](hmmimg/bw4.png)\n",
    "\n",
    "### Estimate Emission Probabilities:\n",
    "\n",
    "Same way, following should be our Emission Probability Matrix.\n",
    "\n",
    "![alt](hmmimg/bw5.png)\n",
    "\n",
    "Here are the calculated probabilities:\n",
    "\n",
    "![alt](hmmimg/bw6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baum-Welch Algorithm:\n",
    "\n",
    "The above maximum likelihood estimate will work only when the sequence of hidden states are known. However thats not the case for us. Hence we need to find another way to estimate the Transition and Emission Matrix.\n",
    "\n",
    "This algorithm is also known as Forward-Backward or Baum-Welch Algorithm, it‚Äôs a special case of the Expectation Maximization (EM) algorithm.\n",
    "\n",
    "High Level Steps of the Algorithm (EM):\n",
    "\n",
    "Lets first understand what we need in order to get an estimate for the parameters of the HMM. Here are the high level steps:\n",
    "\n",
    "1.    Start with initial probability estimates [A,B]. Initially set equal probabilities or define them randomly.\n",
    "2.    Compute expectation of how often each transition/emission has been used. We will estimate latent variables [ ùúâ,ùõæ ] (This is common approach for EM Algorithm)\n",
    "3.    Re-estimate the probabilities [A,B] based on those estimates (latent variable).\n",
    "4.    Repeat until convergence\n",
    "\n",
    "\n",
    "\n",
    "### How to solve Baum-Welch Algorithm?:\n",
    "\n",
    "There are two main ways we can solve the Baum-Welch Algorithm.\n",
    "\n",
    "1.    Probabilistic Approach : HMM is a Generative model, hence we can solve Baum-Welch using Probabilistic Approach.\n",
    "2.    Lagrange Multipliers : The Learning problem can be defined as a constrained optimization problem, hence it can also be solved using Lagrange Multipliers.\n",
    "\n",
    "The final equation for both A, B will look the same irrespective of any of the above approach since both A,B can be defined using joint and marginal probabilities. Let‚Äôs look at the formal definition of them :\n",
    "\n",
    "![alt](hmmimg/bw7.png)\n",
    "\n",
    "The above definition is just the generalized view of the Maximum Likelihood Example we went through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
