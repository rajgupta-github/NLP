{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention\n",
    "\n",
    "Self-attention, also known as intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.\n",
    "\n",
    "The <a href=\"https://arxiv.org/pdf/1601.06733.pdf\" target=\"_blank\">Long short-term memory</a> network paper used self-attention to do machine reading. In the example below, the self-attention mechanism enables us to learn the correlation between the current words and the previous part of the sentence.\n",
    "\n",
    "![title](img/sa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is self attention?\n",
    "\n",
    "> **Self-attention is similar to attention**\n",
    "\n",
    "A self-attention module takes in n inputs, and returns n outputs. What happens in this module? In layman’s terms, the self-attention mechanism allows the inputs to interact with each other (“self”) and find out who they should pay more attention to (“attention”). The outputs are aggregates of these interactions and attention scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical operations involved in a self-attention module\n",
    "\n",
    "#### Step By Step\n",
    "\n",
    "    1. Prepare inputs\n",
    "    2. Initialise weights\n",
    "    3. Derive key, query and value\n",
    "    4. Calculate attention scores for Input 1\n",
    "    5. Calculate softmax\n",
    "    6. Multiply scores with values\n",
    "    7. Sum weighted values to get Output 1\n",
    "    8. Repeat steps 4–7 for Input 2 & Input 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Prepare Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we start with 3 inputs, each with dimension 4.\n",
    "\n",
    "![title](img/sa2.png)\n",
    "\n",
    "\n",
    "    Input 1: [1, 0, 1, 0] \n",
    "    Input 2: [0, 2, 0, 2]\n",
    "    Input 3: [1, 1, 1, 1]\n",
    "\n",
    "#### 2: Initialise weights\n",
    "\n",
    "Every input must have three representations (see diagram below). These representations are called key (orange), query (red), and value (purple). For this example, let’s take that we want these representations to have a dimension of 3. Because every input has a dimension of 4, this means each set of the weights must have a shape of 4×3.\n",
    "\n",
    "\n",
    "In order to obtain these representations, every input (green) is multiplied with a set of weights for keys, a set of weights for querys (I know that’s not the right spelling), and a set of weights for values. \n",
    "\n",
    "\n",
    "In our example, we initialise the three sets of weights as follows.\n",
    "\n",
    "Weights for key:\n",
    "\n",
    "    [[0, 0, 1],\n",
    "     [1, 1, 0],\n",
    "     [0, 1, 0],\n",
    "     [1, 1, 0]]\n",
    "\n",
    "Weights for query:\n",
    "\n",
    "    [[1, 0, 1],\n",
    "     [1, 0, 0],\n",
    "     [0, 0, 1],\n",
    "     [0, 1, 1]]\n",
    "\n",
    "Weights for value:\n",
    "\n",
    "    [[0, 2, 0],\n",
    "     [0, 3, 0],\n",
    "     [1, 0, 3],\n",
    "     [1, 1, 0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3: Derive key, query and value\n",
    "\n",
    "Now that we have the three sets of weights, let’s actually obtain the key, query and value representations for every input.\n",
    "\n",
    "Key representation for Input 1:\n",
    "\n",
    "                   [0, 0, 1]\n",
    "    [1, 0, 1, 0] x [1, 1, 0] = [0, 1, 1]\n",
    "                   [0, 1, 0]\n",
    "                   [1, 1, 0]\n",
    "\n",
    "Use the same set of weights to get the key representation for Input 2:\n",
    "\n",
    "                   [0, 0, 1]\n",
    "    [0, 2, 0, 2] x [1, 1, 0] = [4, 4, 0]\n",
    "                   [0, 1, 0]\n",
    "                   [1, 1, 0]\n",
    "\n",
    "Use the same set of weights to get the key representation for Input 3:\n",
    "\n",
    "                   [0, 0, 1]\n",
    "    [1, 1, 1, 1] x [1, 1, 0] = [2, 3, 1]\n",
    "                   [0, 1, 0]\n",
    "                   [1, 1, 0]\n",
    "\n",
    "A faster way is to vectorise the above operations:\n",
    "\n",
    "                   [0, 0, 1]\n",
    "    [1, 0, 1, 0]   [1, 1, 0]   [0, 1, 1]\n",
    "    [0, 2, 0, 2] x [0, 1, 0] = [4, 4, 0]\n",
    "    [1, 1, 1, 1]   [1, 1, 0]   [2, 3, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/sa3.gif)\n",
    "\n",
    "Let’s do the same to obtain the value representations for every input:\n",
    "\n",
    "                   [0, 2, 0]\n",
    "    [1, 0, 1, 0]   [0, 3, 0]   [1, 2, 3] \n",
    "    [0, 2, 0, 2] x [1, 0, 3] = [2, 8, 0]\n",
    "    [1, 1, 1, 1]   [1, 1, 0]   [2, 6, 3]\n",
    "    \n",
    "    \n",
    "![title](img/sa4.gif)    \n",
    "    \n",
    "and finally the query representations:\n",
    "\n",
    "                   [1, 0, 1]\n",
    "    [1, 0, 1, 0]   [1, 0, 0]   [1, 0, 2]\n",
    "    [0, 2, 0, 2] x [0, 0, 1] = [2, 2, 2]\n",
    "    [1, 1, 1, 1]   [0, 1, 1]   [2, 1, 3]\n",
    "    \n",
    "    \n",
    "![title](img/sa5.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Calculate attention scores for Input 1\n",
    "\n",
    "![title](img/sa6.gif)\n",
    "\n",
    "\n",
    "To obtain attention scores, we start off with taking a dot product between Input 1’s query (red) with all keys (orange), including itself. Since there are 3 key representations (because we have 3 inputs), we obtain 3 attention scores (blue).\n",
    "\n",
    "                [0, 4, 2]\n",
    "    [1, 0, 2] x [1, 4, 3] = [2, 4, 4]\n",
    "                [1, 0, 1]\n",
    "\n",
    "This above operation is known as **dot product attention**.\n",
    "\n",
    "Notice that we only use the query from Input 1. Later we’ll work on repeating this same step for the other querys.\n",
    "\n",
    "\n",
    "Step 5: Calculate softmax\n",
    "\n",
    "![title](img/sa7.gif)\n",
    "\n",
    "Take the softmax across these attention scores (blue).\n",
    "\n",
    "    softmax([2, 4, 4]) = [0.0, 0.5, 0.5]\n",
    "    \n",
    "    \n",
    "Step 6: Multiply scores with values\n",
    "\n",
    "![title](img/sa8.gif)\n",
    "\n",
    "\n",
    "The softmaxed attention scores for each input (blue) is multiplied with its corresponding value (purple). This results in 3 alignment vectors (yellow). In this tutorial, we’ll refer to them as weighted values.\n",
    "\n",
    "    1: 0.0 * [1, 2, 3] = [0.0, 0.0, 0.0]\n",
    "    2: 0.5 * [2, 8, 0] = [1.0, 4.0, 0.0]\n",
    "    3: 0.5 * [2, 6, 3] = [1.0, 3.0, 1.5]\n",
    "\n",
    "\n",
    "\n",
    "Step 7: Sum weighted values to get Output 1\n",
    "\n",
    "![title](img/sa9.gif)\n",
    "\n",
    "Take all the weighted values (yellow) and sum them element-wise:\n",
    "\n",
    "  [0.0, 0.0, 0.0]\n",
    "+ [1.0, 4.0, 0.0]\n",
    "+ [1.0, 3.0, 1.5]\n",
    "-----------------\n",
    "= [2.0, 7.0, 1.5]\n",
    "\n",
    "The resulting vector [2.0, 7.0, 1.5] (dark green) is Output 1, which is based on the query representation from Input 1 interacting with all other keys, including itself.\n",
    "\n",
    "Step 8: Repeat for Input 2 & Input 3\n",
    "\n",
    "Now that we’re done with Output 1, we repeat Steps 4 to 7 for Output 2 and Output 3. I trust that I can leave you to work out the operations yourself\n",
    "\n",
    "Finally!!!\n",
    "\n",
    "![title](img/sa10.gif)\n",
    "\n",
    ">Notes : - \n",
    ">The dimension of query and key must always be the same because of the dot product score function. However, the dimension of value may be different from query and key. The resulting output will consequently follow the dimension of value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
