{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the essence of attention?\n",
    "\n",
    ">The Attention mechanism, if understood shallowly, matches his name very well. His core logic is \" from focusing on everything to focusing on the focus .\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Attention mechanism is very similar to the logic of humans looking at pictures. When we look at a picture, we do not see the entire content of the picture, but focus on the focus of the picture. Take a look at the following picture:\n",
    "\n",
    "![title](img/a1.jpg)\n",
    "\n",
    "We will definitely see the 4 words of \"Jinjiang Hotel\", as shown below:\n",
    "\n",
    "![title](img/a2.jpg)\n",
    "\n",
    "So when we look at a picture, it actually looks like this:\n",
    "\n",
    "![title](img/a3.jpg)\n",
    "\n",
    "As mentioned above, our vision system is an Attention mechanism that focuses limited attention on key information, thereby saving resources and quickly obtaining the most effective information.\n",
    "\n",
    "\n",
    "Attention mechanism in the AI ​​field\n",
    "\n",
    "The Attention mechanism was first applied in computer vision, and then began to be applied in the NLP field. The true development is in the NLP field, because  the effects of BERT and GPT in 2018  were surprisingly good, and became popular. The  Transformer  and Attention to these core began to be of our focus.\n",
    "\n",
    "If the position of Attention is expressed by a picture, it looks like this:\n",
    "\n",
    "![title](img/a4.png)\n",
    "\n",
    "#### 3 advantages of Attention\n",
    "There are three main reasons to introduce the Attention mechanism:\n",
    "\n",
    "1. Less parameters\n",
    "2. High speed\n",
    "3. Works well\n",
    "\n",
    "\n",
    "**Less parameters**\n",
    "\n",
    "Compared with CNN and RNN , the model complexity is smaller and the parameters are fewer. So the requirement for computing power is even smaller.\n",
    "\n",
    "**High speed**\n",
    "\n",
    "Attention solves the problem that RNN cannot be calculated in parallel. Each calculation of the Attention mechanism does not depend on the calculation result of the previous step, so it can be processed in parallel with CNN.\n",
    "\n",
    "**Works well**\n",
    "\n",
    "Prior to the introduction of the Attention mechanism, there was a problem that everyone was very distressed: long-distance information would be weakened, just like people with weak memory ability, can’t remember the past.\n",
    "\n",
    "Attention is to pick the point, even if the text is relatively long, you can grab the point in the middle without losing important information. The red expectations in the picture below are the points that have been singled out.\n",
    "\n",
    "![title](img/a5.jpg)\n",
    "\n",
    "### The principle of Attention\n",
    "\n",
    "Attention is often said with Encoder - Decoder .\n",
    "\n",
    "The following animation shows the general flow of completing machine translation tasks when attention is introduced into the Encoder-Decoder framework.\n",
    "\n",
    "![title](img/a6.gif)\n",
    "\n",
    "**However, Attention does not have to be used under the Encoder-Decoder framework, it can be separated from the Encoder-Decoder framework.**\n",
    "\n",
    "### 3-step decomposition of the Attention principle:\n",
    "\n",
    "- Step 1: Calculate the similarity of query and key to get the weight\n",
    "\n",
    "- Step 2: Normalize the weights to get directly available weights\n",
    "\n",
    "- Step 3: Weighted sum of weight and value\n",
    "\n",
    "### N types of attention\n",
    "\n",
    "There are many different types of Attention: Soft Attention, Hard Attention, Static Attention, Dynamic Attention, Self Attention, and so on. Let me explain to you the differences between these different Attentions.\n",
    "\n",
    "Part II: Attention form\n",
    "\n",
    "This section categorizes the form of Attention from the areas of calculation, information used, structural levels, and models.\n",
    "\n",
    "1. **Calculation area**\n",
    "\n",
    "According to the calculation area of ​​Attention, it can be divided into the following types:\n",
    "\n",
    "1) Soft Attention. This is a more common Attention method. The weight probability is calculated for all keys. Each key has a corresponding weight. It is a global calculation method (also called Global Attention). This method is more rational, referring to the contents of all keys, and then weighting. However, the amount of calculation may be relatively large.\n",
    "\n",
    "2) Hard Attention. This method is to directly locate a certain key accurately, and the rest of the keys are ignored, which is equivalent to the probability of this key being 1, and the probability of all other keys being all 0. Therefore, this alignment method is very demanding and requires one step. If it is not properly aligned, it will have a great impact. On the other hand, because it is undifferentiable, it is generally necessary to use reinforcement learning to train. (Or use gumbel softmax or something)\n",
    "\n",
    "3) Local Attention, this method is actually a compromise between the above two methods, calculating a window area. First use Hard method to locate a certain place, and use this point as the center to get a window area. In this small area, use Soft method to calculate Attention.\n",
    "\n",
    "2. **Information Used**\n",
    "\n",
    "Suppose we want to calculate Attention for a piece of original text. Here the original text refers to the text we want to pay attention to, then the information used includes internal information and external information, internal information refers to the original text itself, and external information refers to the original text. Additional information.\n",
    "\n",
    "1) General  Attention. This method uses external information and is often used for tasks that need to construct two text relationships. The query generally contains additional information and the original text is aligned according to the external query.\n",
    "\n",
    "For example, in the task of reading comprehension, you need to construct the relationship between the problem and the article. Suppose now that the baseline is to calculate a problem vector q for the problem , stitch this q with all the article word vectors, and input it into the LSTM for modeling. So in this model, all the word vectors of the article share the same problem vector. Now we want to make the word vectors of each step of the article have a different problem vector. The problem is the attention, the problem here belongs to the original text, and the article word vector belongs to the external information.\n",
    "\n",
    "2) Local  Attention. This method only uses internal information. The key and value and query are only related to the input text. In self attention, key = value = query. Since there is no external information, each word in the original text can be subjected to Attention calculation with all the words in the sentence, which is equivalent to finding the relationship inside the original text.\n",
    "\n",
    "Let's also take the example of reading comprehension task. As mentioned in the above baseline, a vector q is calculated for the problem, then attention can also be used here, and only the information of the problem itself is used to do attention without introducing article information.\n",
    "\n",
    " \n",
    "\n",
    "3. **Structure level**\n",
    "\n",
    "The structural aspect is divided into single-layer attention, multi-layer attention and multi-head attention according to whether the hierarchical relationship is divided:\n",
    "\n",
    "1) Single-layer Attention. This is a relatively common practice. Use a query to pay attention to a piece of original text.\n",
    "\n",
    "2) Multi-layer attention, which is generally used in models with hierarchical relationships in text. Suppose we divide a document into multiple sentences. At the first layer, we use attention to calculate a sentence vector (that is, a single layer) for each sentence. attention); in the second layer, we do attention to all sentence vectors to calculate a document vector (also a single layer of attention), and finally use this document vector to do the task.\n",
    "\n",
    "3) Multi-head Attention. This is the multi-head attention mentioned in Attention is All You Need. It uses multiple queries to pay attention to a piece of text multiple times. Each query pays attention to different parts of the original text, which is equivalent to repeating. Multiple single-layer attentions:\n",
    "\n",
    "![title](img/a7.jpg)\n",
    "\n",
    "Finally, stitch these results together:\n",
    "\n",
    "![title](img/a8.jpg)\n",
    "\n",
    "4. **Model aspect**\n",
    "\n",
    "From the model point of view, Attention is generally used on CNN and LSTM, and pure Attention calculation can also be performed directly.\n",
    "\n",
    "1) CNN + Attention\n",
    "\n",
    "CNN's convolution operation can extract important features. I think this is also the idea of Attention, but CNN's convolution perception field of view is local, and it is necessary to expand the field of view by superimposing multiple convolution regions. In addition, Max Pooling directly extracts the feature with the largest value, which is also like the idea of hard attention, and directly selects a feature.\n",
    "\n",
    "Adding Attention to CNN can be added in these areas:\n",
    "\n",
    "a. Do attention before the convolution operation, such as Attention-Based BCNN-1. This task is a text implication task that needs to process two pieces of text. At the same time, attention is paid to the two input sequence vectors, the feature vector is calculated, and then spliced to the original. Vector as input to the convolutional layer.\n",
    "\n",
    "b. Attention after the convolution operation, such as Attention-Based BCNN-2, take attention to the output of the convolution layer of two texts as the input of the pooling layer.\n",
    "\n",
    "c. Do attention at the pooling layer instead of max pooling. For example, Attention pooling, we first use LSTM to learn a better sentence vector as a query, and then use CNN to learn a feature matrix as the key, and then use query to generate weights for the key, and then pay attention to the final sentence vector.\n",
    "\n",
    " \n",
    "\n",
    "2) LSTM + Attention\n",
    "\n",
    "There is a Gate mechanism inside the LSTM, where the input gate chooses which current information to enter, and the forget gate chooses which past information is forgotten. I think this is a certain degree of attention, and it claims to solve the long-term dependency problem. In fact, LSTM needs to capture step by step The performance of sequence information on long texts will gradually decay as the step increases, making it difficult to retain all useful information.\n",
    "\n",
    "LSTM usually needs to get a vector and then do the task. The common methods are:\n",
    "\n",
    "a. Use the last hidden state directly (some previous information may be lost and it is difficult to express the full text)\n",
    "\n",
    "b. Equal weighting the hidden state of all steps (equal treatment of all steps).\n",
    "\n",
    "c. Attention mechanism, weights the hidden state of all steps, and focuses attention on the more important hidden state information in the entire text. The performance is better than the previous two, and it is convenient to visually observe which steps are important, but be careful of overfitting, and it also increases the amount of calculation.\n",
    "\n",
    " \n",
    "\n",
    "3) Pure Attention\n",
    "\n",
    "Attention is all you need, CNN / RNN is not used, and it is a clear stream at first glance, but if you look closely, it is essentially a bunch of vectors to calculate attention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a_2__j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Intution\n",
    "\n",
    "In the Encoder-Decoder structure, Encoder encodes all input sequences into a unified semantic feature c and then decodes it. Therefore, c(Context vector) must contain all the information in the original sequence, and its length becomes a bottleneck that limits the performance of the model. For example, in the case of machine translation, when a sentence to be translated is long, a single c may not store so much information, which will cause a reduction in translation accuracy.\n",
    "\n",
    "The Attention mechanism solves this problem by entering a different c at each time. The following figure is a Decoder with an Attention mechanism:\n",
    "\n",
    "![title](img/a9.jpg)\n",
    "\n",
    "Each c will automatically select the most appropriate context information for the current y to be output. In particular, we measure the correlation Encoder hj and decoding the j-th stage the i-th stage, the final Decoder context information inputted i-th stage [formula]it from all [formula]of the [formula]weighted sum.\n",
    "\n",
    "\n",
    "Take machine translation as an example (translate Chinese into English):\n",
    "\n",
    "![title](img/a10.jpg)\n",
    "\n",
    "The input sequence is \"I love China\", so h1, h2, h3, h4 in Encoder can be regarded as the information represented by \"I\", \"Love\", \"Medium\", and \"Country\", respectively. When translated into English, the first context should c1 and \"I\" is the word most relevant, and therefore the corresponding [formula]relatively large, while the corresponding [formula], [formula], [formula]is relatively small. c2 should be \"love\" the most relevant, and therefore the corresponding [formula]relatively large. Last c3 and h3, h4 most relevant, therefore [formula], [formula]the value is relatively large.\n",
    "\n",
    "\n",
    "So far, about Attention model, we are left with one last question, and that is: these weights [formula]is how come?\n",
    "\n",
    "In fact, [formula]it is also learned from the model, which is actually related to the hidden state of the i-1 stage of the Decoder and the hidden state of the j stage of the Encoder.\n",
    "\n",
    "Or take the same example above, machine translation, [formula]calculated (when it is expressed by arrow h ', and [formula]while doing transformation):\n",
    "\n",
    "\n",
    "![title](img/a10.jpg)\n",
    "\n",
    "$a_2j$ Calculation:\n",
    "\n",
    "![title](img/a10.jpg)\n",
    "\n",
    "$a_3j$ Calculation:\n",
    "![title](img/a10.jpg)\n",
    "\n",
    "\n",
    "The above is the whole process of Encoder-Decoder model calculation with Attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Intution\n",
    "\n",
    "In the Encoder-Decoder structure, Encoder encodes all input sequences into a unified semantic feature c and then decodes it. Therefore, c must contain all the information in the original sequence, and its length becomes a bottleneck that limits the performance of the model. For example, in the case of machine translation, when a sentence to be translated is long, a single c may not store so much information, which will cause a reduction in translation accuracy.\n",
    "\n",
    "The Attention mechanism solves this problem by entering a different c at each time. The following figure is a Decoder with an Attention mechanism:\n",
    "\n",
    "![title](img/a9.jpg)\n",
    "\n",
    "Each c will automatically select the most appropriate context information for the current y to be output. In particular, we $a_2j$ measure the correlation Encoder hj and decoding the j-th stage the i-th stage, the final Decoder context information inputted i-th stage $c_i$ it from all $h_j$ of the $a_ij$ weighted sum.\n",
    "\n",
    "\n",
    "Take machine translation as an example (translate Chinese into English):\n",
    "\n",
    "![title](img/a10.jpg)\n",
    "\n",
    "The input sequence is \"I love China\", so h1, h2, h3, h4 in Encoder can be regarded as the information represented by \"I\", \"Love\", \"Medium\", and \"Country\", respectively. When translated into English, the first context should c1 and \"I\" is the word most relevant, and therefore the corresponding $a_11$ relatively large, while the corresponding $a_12$, $a_13$, $a_14$ is relatively small. c2 should be \"love\" the most relevant, and therefore the corresponding $a_22$ relatively large. Last c3 and h3, h4 most relevant, therefore $a_33$, $a_34$ the value is relatively large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, about Attention model, we are left with one last question, and that is: these weights $a_ij$ is how come?\n",
    "\n",
    "In fact, $a_ij$ it is also learned from the model, which is actually related to the hidden state of the i-1 stage of the Decoder and the hidden state of the j stage of the Encoder.\n",
    "\n",
    "Or take the same example above, machine translation, $a_1j$ calculated (when it is expressed by arrow h ', and $h_j$ while doing transformation):\n",
    "\n",
    "\n",
    "![title](img/a11.jpg)\n",
    "\n",
    "$a_2j$ Calculation:\n",
    "\n",
    "![title](img/a12.jpg)\n",
    "\n",
    "$a_3j$ Calculation:\n",
    "![title](img/a13.jpg)\n",
    "\n",
    "\n",
    "The above is the whole process of Encoder-Decoder model calculation with Attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
